{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender (based on the imdb top 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the movie title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name_given_by_user = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = 'https://www.imdb.com/find'\n",
    "payload = {'q': movie_name_given_by_user}\n",
    "soup_search = BeautifulSoup(requests.get(search_url, params=payload).text, 'html.parser')\n",
    "movie_found = soup_search.select(\"a[href*='/title/tt']\")\n",
    "\n",
    "if movie_found == []:\n",
    "    raise SystemExit(f'Filme \\\"{movie_name_given_by_user}\\\" nÃ£o encontrado no Imdb. Verifique e tente novamente.')\n",
    "\n",
    "# Assumes that the first result of the imdb search is the correct one\n",
    "imdb_id_given_by_user = str.replace(movie_found[0].attrs['href'], \"/title/\", \"\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb_top_1000.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The user choose a movie to compare. This movie is in the Top 1000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we didn't find the movie in the DataFrame -> Get movie data from imdb.com\n",
    "if df[ df['imdb_id'] == imdb_id_given_by_user ].shape[0] == 0:\n",
    "    genres = []\n",
    "    url = 'https://www.imdb.com/title/' + imdb_id_given_by_user\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "\n",
    "    title = soup.find('h1').text\n",
    "    plot = soup.find('span', attrs={'data-testid': 'plot-xl'}).text\n",
    "    director = soup.select(\"a[href*='/name/nm']\")[0].text\n",
    "    year = soup.select(\"a[href*='/releaseinfo?ref_=tt_ov_rdat']\")[0].text\n",
    "    genres_soup = soup.select(\"a[href*='/search/title?genres=']\")\n",
    "    for genre in genres_soup:\n",
    "        genres.append(genre.span.text)\n",
    "    genres = \", \".join(genres)\n",
    "    rate = float(soup.find('div', {'data-testid': 'hero-rating-bar__aggregate-rating__score'}).span.text)\n",
    "\n",
    "    # Adding the new movie to the DataFrame\n",
    "    new_movie = pd.DataFrame([{\n",
    "                    'imdb_id': imdb_id_given_by_user,\n",
    "                    'title': title,\n",
    "                    'rate': rate,\n",
    "                    'year': year,\n",
    "                    'director': director,\n",
    "                    'genres': genres,\n",
    "                    'plot': plot }])\n",
    "\n",
    "    df = pd.concat([df, new_movie], ignore_index=True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating 'genres' and 'director' with 'plot' to tokenize later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we dupplicate the director name so it has more weight comparing two movies\n",
    "df['combined_features'] = df['title'] + \" \" + df['director'] + \" \" + \\\n",
    "                    df['genres'].apply(lambda x: str.replace(x, '\\'', '')) + \\\n",
    "                    \" \" + df['plot']\n",
    "df['combined_features'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that tokenize and stem a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize to perform the cosine distance analisys. We also stem the text to transform the words into their root form, so words like \"run\" and \"running\" are grouped in the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an English language SnowballStemmer object\n",
    "stemmer_en = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    \n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = [ word for sent in sent_tokenize(text) for word in word_tokenize(sent) ]\n",
    "    \n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z0-9]', token)]\n",
    "    \n",
    "    # Stem the filtered_tokens\n",
    "    stems = [ stemmer_en.stem(ft) for ft in filtered_tokens ]\n",
    "    \n",
    "    return stems\n",
    "\n",
    "# testing the function:\n",
    "# print( tokenize_and_stem(\"[Drama, Romance, War] At a U.S. Army base at 1945\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the movie plot and genres with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer( max_features=50000, stop_words='english',\n",
    "                                    tokenizer=tokenize_and_stem, ngram_range=(1,2) )\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_features'].values)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "print(cosine_sim[0:4,0:4], cosine_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_index = df[ df['imdb_id'] == imdb_id_given_by_user ].index\n",
    "\n",
    "# Find the cosine similarity tax with all the movies from the df, addint the index in the tupple\n",
    "similar_movies = list(enumerate(cosine_sim[movie_index,:][0]))\n",
    "\n",
    "# Sort the result, in descending order\n",
    "sorted_similar_movies = sorted(similar_movies, key=lambda x:x[1], reverse=True)\n",
    "sorted_similar_movies[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the data from the 10 movies more simmilar to the user's choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_indexes = [ m[0] for m in sorted_similar_movies[:15] ]\n",
    "top_15_scores = [ m[1] for m in sorted_similar_movies[:15] ]\n",
    "top_15_sim = df.iloc[top_15_indexes].drop(['combined_features'], axis=1)\n",
    "top_15_sim['similarity'] = top_15_scores\n",
    "top_15_sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44e092dbf5a2d7f854204366b602d774237af8617f169b3b06803da90d84e814"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
